model_id,architecture_type,IsDeepSeekBase,IsDeepSeekTrained,IsLlama70Base,IsQwen72Base,IsLlama405Base,total_params,active_params,mlp_neurons_total,Params/stream,cache_compression,key_head_diversity,stream_compress,num_hidden_layers,num_dense_layers,hidden_size,mlp_intermediate_size,num_experts * moe_intermediate size,moe_intermediate_size,num_local_experts,num_experts_per_tok,n_shared_experts,num_attention_heads,num_key_value_heads,head_dim,num_mtp_modules,hidden_act,vocab_size,context_length,rope_theta,attention_mechanism,positional_encoding,normalization,attention_bias,training_tokens,KVLoRA_rank,QLoRA_rank,share_q_dim,first_k_dense_replace,,release_date,notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deepseek/deepseek-r1-distill-qwen-32b,Dense,0,1,0,0,0,"32,000,000,000","32,000,000,000","1,769,472","6,250,000",0.200,0.008,0.185,64,64,5120,27648,27648,,,,,40,8,128,,silu,152064,131072,1000000.0,GQA,RoPE (Rotary),RMSNorm,,,,,,,,"January 19-20, 2025",Uses sliding window attention,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
z-ai/glm-4-32b,Dense,0,0,0,0,0,"32,000,000,000","32,000,000,000","1,405,440","5,208,333.33333333",0.042,0.002,0.267,61,61,6144,23040,23040,,,,,48,2,128,,silu,151552,32768,10000.0,GQA,RoPE (Rotary),RMSNorm,FALSE,,,,,,,"April 14, 2025",Extreme GQA: only 2 KV heads for 48 query heads; Partial RoPE with factor 0.5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nvidia/OpenReasoning-Nemotron-32B:featherless-ai,Dense,0,0,0,0,0,"32,000,000,000","32,000,000,000","1,769,472","6,250,000",0.200,0.008,0.185,64,64,5120,27648,27648,,,,,40,8,128,,silu,152064,131072,1000000.0,GQA,RoPE (Rotary),RMSNorm,,,,,,,,"July 18, 2025","Qwen2.5-32B derivative; Reasoning model for math, code and science",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
agentica-org/DeepSWE-Preview:featherless-ai,Dense,0,0,0,0,0,"32,000,000,000","32,000,000,000","1,638,400","6,250,000",0.125,0.008,0.200,64,64,5120,25600,25600,,,,,64,8,128,,silu,151936,40960,1000000,GQA,RoPE (Rotary),RMSNorm,FALSE,,,,,,,"July 1, 2025",Qwen3 architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MiniMaxAI/SynLogic-Mix-3-32B:featherless-ai,Dense,0,0,0,0,0,"32,000,000,000","32,000,000,000","1,769,472","6,250,000",0.200,0.008,0.185,64,64,5120,27648,27648,,,,,40,8,128,,silu,152064,131072,1000000.0,GQA,RoPE (Rotary),RMSNorm,,,,,,,,Jun 3,Qwen2.5-32B based,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
qwen/qwen3-32b,Dense,0,0,0,0,0,"32,000,000,000","32,000,000,000","1,638,400","6,250,000",0.125,0.008,0.200,64,64,5120,25600,25600,,,,,64,8,128,,silu,151936,40960,1000000,GQA,RoPE (Rotary),RMSNorm,FALSE,,,,,,,"April 28, 2025",Qwen3ForCausalLM architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
arcee-ai/Arcee-SuperNova-v1:featherless-ai,Dense,0,0,1,0,0,"70,000,000,000","70,000,000,000","2,293,760","8,544,921.875",0.125,0.008,0.286,80,80,8192,28672,28672,,,,,64,8,128,,silu,128256,131072,500000.0,GQA,RoPE (Rotary),RMSNorm,FALSE,,,,,,,Jun 10,Llama 3.1 70B based; RoPE scaling llama3 factor 8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deepseek/deepseek-r1-distill-llama-70b,Dense,0,1,1,0,0,"70,000,000,000","70,000,000,000","2,293,760","8,544,921.875",0.125,0.008,0.286,80,80,8192,28672,28672,,,,,64,8,128,,silu,128256,131072,500000.0,GQA,RoPE (Rotary),RMSNorm,FALSE,,,,,,,"January 20, 2025",Llama architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
thedrummer/anubis-70b-v1.1,Dense,0,0,1,0,0,"70,000,000,000","70,000,000,000","2,293,760","8,544,921.875",0.125,0.008,0.286,80,80,8192,28672,28672,,,,,64,8,128,,silu,128256,131072,500000.0,GQA,RoPE (Rotary),RMSNorm,FALSE,,,,,,,"January 8, 2025",Llama architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nousresearch/hermes-4-70b,Dense,0,0,1,0,0,"70,000,000,000","70,000,000,000","2,293,760","8,544,921.875",0.125,0.008,0.286,80,80,8192,28672,28672,,,,,64,8,128,,silu,128256,131072,500000.0,GQA,RoPE (Rotary),RMSNorm,FALSE,,,,,,,"August 26, 2025",Llama architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deepcogito/cogito-v2-preview-llama-70b,Dense,0,0,1,0,0,"70,000,000,000","70,000,000,000","2,293,760","8,544,921.875",0.125,0.008,0.286,80,80,8192,28672,28672,,,,,64,8,128,,silu,128256,131072,500000.0,GQA,RoPE (Rotary),RMSNorm,FALSE,,,,,,,"September 2, 2025",Llama architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AI-MO/Kimina-Prover-72B:featherless-ai,Dense,0,0,0,1,0,"72,000,000,000","72,000,000,000","2,365,440","8,789,062.5",0.125,0.008,0.277,80,80,8192,29568,29568,,,,,64,8,128,,silu,152064,131072,1000000.0,GQA,RoPE (Rotary),RMSNorm,,,,,,,,"April 14, 2025",Qwen2.5-72B based,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
moonshotai/Kimi-Dev-72B:featherless-ai,Dense,0,0,0,1,0,"72,000,000,000","72,000,000,000","2,365,440","8,789,062.5",0.125,0.008,0.277,80,80,8192,29568,29568,,,,,64,8,128,,silu,152064,131072,1000000.0,GQA,RoPE (Rotary),RMSNorm,,,,,,,,"June 17, 2025",Qwen2.5-72B based,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unbabel/Tower-Plus-72B:featherless-ai,Dense,0,0,0,1,0,"72,000,000,000","72,000,000,000","2,365,440","8,789,062.5",0.125,0.008,0.277,80,80,8192,29568,29568,,,,,64,8,128,,silu,152064,32768,10000.0,GQA,RoPE (Rotary),RMSNorm,,,,,,,,"June 20, 2025",Qwen2.5-72B based; Translation model,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
qwen/qwen-2.5-72b-instruct,Dense,0,0,0,1,0,"72,000,000,000","72,000,000,000","2,365,440","8,789,062.5",0.125,0.008,0.277,80,80,8192,29568,29568,,,,,64,8,128,,silu,152064,32768,1000000.0,GQA,RoPE (Rotary),RMSNorm,,,,,,,,"September 19, 2024",Qwen2ForCausalLM architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
anthracite-org/magnum-v4-72b,Dense,0,0,0,1,0,"72,000,000,000","72,000,000,000","2,365,440","8,789,062.5",0.125,0.008,0.277,80,80,8192,29568,29568,,,,,64,8,128,,silu,152064,32768,1000000.0,GQA,RoPE (Rotary),RMSNorm,,,,,,,,"October 21, 2024",Qwen2ForCausalLM architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inclusionai/ring-flash-2.0,Mixture-of-experts,0,0,0,0,0,"100,000,000,000","6,000,000,000","8,388,608","24,414,062.5",0.125,0.004,0.500,32,,4096,9216,9216,1024,256,8,1,32,4,128,,silu,157184,32768,600000,GQA,RoPE (Rotary),RMSNorm,,,,,,,,"September 29, 2025",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
z-ai/glm-4.5-air,Mixture-of-experts,0,0,0,0,0,"106,000,000,000","12,000,000,000","8,290,304","25,878,906.25",0.083,0.008,0.416,46,,4096,10944,11264,1408,128,7,1,96,8,128,,silu,151552,131072,1000000,GQA,RoPE (Rotary),RMSNorm,TRUE,,,,,,,"July 25, 2025",num_local_experts from n_routed_experts; Has 1 shared expert(s),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/llama-4-scout,Mixture-of-experts,0,0,0,0,0,"109,000,000,000","17,000,000,000","6,291,456","21,289,062.5",0.200,0.008,0.625,48,,5120,8192,8192,8192,16,1,0,40,8,128,,silu,202048,10485760,500000.0,GQA,RoPE (Rotary),RMSNorm,FALSE,,,,,,,"April 5, 2025","Multimodal model (vision + text); Vision encoder: 34 layers, 1408 hidden size; MoE intermediate: 8192, MLP intermediate listed as 16384 in config but not used in architecture according to Manus research; Interleaved MoE layers (step: 1); RoPE scaling: llama3 with factor 16.0; Uses QK normalization",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deepcogito/cogito-v2-preview-llama-109b,Mixture-of-experts,,0,0,0,0,"109,000,000,000","17,000,000,000","6,291,456","21,289,062.5",0.200,0.008,0.625,48,,5120,16384,8192,8192,16,1,0,40,8,128,,silu,201135,262144,500000,GQA,RoPE (Rotary),RMSNorm,,,,,,,,"September 2, 2025",Llama 4 based; All 48 layers MoE; Hybrid chunked+full attention; Uses QK norm; RoPE scaling llama3 factor 16; Multimodal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
minimax/minimax-m2,Mixture-of-experts,0,0,0,0,0,"230,000,000,000","10,000,000,000","24,379,392","74,869,791.6666667",0.167,0.008,0.250,62,,3072,12288,12288,1536,256,8,0,48,8,128,3,silu,200064,196608,5000000,GQA,RoPE (Rotary),RMSNorm,,,,,,,,"October 27, 2025",Quantized with fp8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
qwen/qwen3-235b-a22b-thinking-2507,Mixture-of-experts,0,0,0,0,0,"235,000,000,000","22,000,000,000","18,481,152","57,373,046.875",0.063,0.004,0.333,94,,4096,12288,12288,1536,128,8,0,64,4,128,,silu,151936,262144,5000000,GQA,RoPE (Rotary),RMSNorm,FALSE,,,,,,,"July 25, 2025",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
baidu/ernie-4.5-300b-a47b,Mixture-of-experts,0,0,0,0,0,"300,000,000,000","47,000,000,000","12,386,304","36,621,093.75",0.125,0.008,0.286,54,,8192,28672,28672,3584,64,8,0,64,8,128,,silu,103424,131072,500000,GQA,RoPE (Rotary),RMSNorm,,,,,,,,"June 30, 2025",MoE layers from 3 to 53; moe_k=8 experts per token,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
z-ai/glm-4.6,Mixture-of-experts,0,0,0,0,0,"355,000,000,000","32,000,000,000","22,609,920","69,335,937.5",0.083,0.008,0.476,92,,5120,12288,12288,1536,160,7,1,96,8,128,,silu,151552,202752,1000000,GQA,RoPE (Rotary),RMSNorm,TRUE,,,,,,,"September 30, 2025",num_local_experts from n_routed_experts; Has 1 shared expert(s),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/llama-4-maverick,Mixture-of-experts,0,0,0,0,0,"400,000,000,000","17,000,000,000","50,331,648","78,125,000",0.200,0.008,0.625,48,,5120,16384,8192,8192,128,1,0,40,8,128,,silu,202048,1048576,500000.0,GQA,RoPE (Rotary),RMSNorm,FALSE,,,,,,,"April 5, 2025","Multimodal model (vision + text); Vision encoder: 34 layers, 1408 hidden size; 128 experts with only 1 active per token (very sparse MoE); MoE intermediate: 8192, MLP intermediate: 16384; Interleaved MoE layers (step: 2)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deepcogito/cogito-v2-preview-llama-405b,Dense,0,0,0,0,1,"405,000,000,000","405,000,000,000","6,709,248","24,719,238.28125",0.063,0.008,0.308,126,126,16384,53248,53248,,,,,128,8,128,,silu,128256,131072,500000,GQA,RoPE (Rotary),RMSNorm,,,,,,,,"October 17, 2025",Llama 3.1 405B based; Dense architecture; RoPE scaling llama3 factor 8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nousresearch/hermes-4-405b,Dense,0,0,0,0,1,"405,000,000,000","405,000,000,000","6,709,248","24,719,238.28125",0.063,0.008,0.308,126,126,16384,53248,53248,,,,,128,8,128,,silu,128256,131072,500000,GQA,RoPE (Rotary),RMSNorm,,,,,,,,"August 26, 2025",Llama 3.1 405B based; Dense architecture; RoPE scaling llama3 factor 8.0; Fine-tuned by NousResearchbaichuan-inc/Baichuan-M2-32B:novita,Dense,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
minimax/minimax-m1,Mixture-of-experts,0,0,0,0,0,"456,000,000,000","46,000,000,000","23,592,960","74,218,750",0.125,0.008,0.333,80,,6144,18432,18432,9216,32,2,0,64,8,128,,silu,200064,10240000,10000000,GQA,RoPE (Rotary),RMSNorm,,,,,,,,"June 17, 2025",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
qwen/qwen3-coder,Mixture-of-experts,0,0,0,0,0,480000000000,"35,000,000,000","158,720","78,125,000",0.083,0.008,0.300,62,,6144,8192,20480,2560,160,8,0,96,8,128,,silu,151936,262144,10000000,GQA,RoPE (Rotary),RMSNorm,,,,,,,,"April 29, 2025",Qwen3 MoE architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inclusionai/ring-1t,Mixture-of-experts,0,0,0,0,0,"1,000,000,000,000","50,000,000,000","41,943,040","122,070,312.5",0.125,0.008,0.500,80,,8192,18432,18432,2048,256,8,1,64,8,128,,silu,157184,32768,600000,GQA,RoPE (Rotary),RMSNorm,,,,,,4,,"September 29, 2025",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meituan/longcat-flash-chat,Mixture-of-experts,0,0,0,0,0,"560,000,000,000","27,000,000,000","29,360,128","91,145,833.3333333",0.031,0.125,0.250,28,,6144,12288,24576,2048,512,12,0,64,64,128,,silu,131072,131072,10000000.0,MLA,RoPE (Rotary),RMSNorm,FALSE,,512,1536,,,,"September 9, 2025","MLA attention with KV LoRA rank 512 and Q LoRA rank 1536; 256 zero experts (type: identity); v_head_dim=128, qk_nope_head_dim=128, qk_rope_head_dim=64; Dynamic MLP intermediate size: 12,288 (min) to 36,864 (max), averaging ~24,576",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deepseek/deepseek-v3.2-exp,Mixture-of-experts,1,1,0,0,0,"671,000,000,000","37,000,000,000","31,981,568","93,610,491.0714286",0.016,0.250,0.438,61,,7168,18432,18432,2048,256,8,1,128,128,128,,silu,129280,163840,10000,MLA,RoPE (Rotary),RMSNorm,FALSE,,512,1536,,3,,"September 29, 2025",head_dim from v_head_dim=128; num_local_experts from n_routed_experts; Has 1 shared expert(s); Uses KV LoRA with rank 512; Uses Q LoRA with rank 1536; Quantized with fp8; RoPE scaling: yarn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deepseek/deepseek-r1,Mixture-of-experts,1,1,0,0,0,"671,000,000,000","37,000,000,000","124,928","93,610,491.0714286",0.016,0.250,0.438,61,,7168,18432,18432,2048,256,8,1,128,128,128,,silu,129280,163840,10000,MLA,RoPE (Rotary),RMSNorm,FALSE,,512,1536,,3,,"January 20, 2025",DeepSeek V3 architecture; KV LoRA rank 512; Q LoRA rank 1536; Quantized with fp8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deepseek/deepseek-chat-v3-0324,Mixture-of-experts,1,1,0,0,0,"671,000,000,000","37,000,000,000","124,928","93,610,491.0714286",0.016,0.250,0.438,61,,7168,18432,18432,2048,256,8,1,128,128,128,,silu,129280,163840,10000,MLA,RoPE (Rotary),RMSNorm,FALSE,,512,1536,,3,,"March 24, 2025",DeepSeek V3 architecture; KV LoRA rank 512; Q LoRA rank 1536; Quantized with fp8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deepseek/deepseek-prover-v2,Mixture-of-experts,1,1,0,0,0,"671,000,000,000","37,000,000,000","124,928","93,610,491.0714286",0.016,0.250,0.438,61,,7168,18432,18432,2048,256,8,1,128,128,128,,silu,129280,163840,10000,MLA,RoPE (Rotary),RMSNorm,FALSE,,512,1536,,3,,"April 30, 2025",DeepSeek V3 architecture; KV LoRA rank 512; Q LoRA rank 1536; Quantized with fp8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deepseek/deepseek-r1-0528,Mixture-of-experts,1,1,0,0,0,"671,000,000,000","37,000,000,000","124,928","93,610,491.0714286",0.016,0.250,0.438,61,,7168,18432,18432,2048,256,8,1,128,128,128,,silu,129280,163840,10000,MLA,RoPE (Rotary),RMSNorm,FALSE,,512,1536,,3,,"May 28, 2025",DeepSeek V3 architecture; KV LoRA rank 512; Q LoRA rank 1536; Quantized with fp8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stepfun-ai/step3,Mixture-of-experts,0,0,0,0,0,"321,000,000,000","38,000,000,000","14,991,360","44,782,366.0714286",0.016,1.000,0.467,61,,7168,18432,20480,5120,48,3,1,64,64,256,,silu,128815,65536,500000,MultiFactor Attention,RoPE (Rotary),RMSNorm,,,,,2048,,,"August 1, 2025","Multimodal model (vision + text); Vision encoder: 63 layers, 1792 hidden size; 56 MoE layers out of 61 total; Has shared expert with dim 5120; share_q_dim=2048 for attention",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deepcogito/cogito-671b-v2.1,Mixture-of-experts,1,0,0,0,0,"671,000,000,000","37,000,000,000","31,981,568","93,610,491.0714286",0.016,0.250,0.438,61,,7168,18432,18432,2048,256,8,1,128,128,128,,silu,128815,163840,10000,MLA,RoPE (Rotary),RMSNorm,,,512,1536,,,,"November 19, 2025",head_dim from v_head_dim=128; Has 1 shared expert; Uses KV LoRA rank 512; Uses Q LoRA rank 1536; RoPE scaling yarn; qk_nope_head_dim=128 qk_rope_head_dim=64; head_dim=64 in config but v_head_dim=128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deepseek-ai/deepseek-v3.1,Mixture-of-experts,1,1,0,0,0,"685,000,000,000","37,000,000,000","31,981,568","95,563,616.0714286",0.016,0.250,0.438,61,,7168,18432,18432,2048,256,8,1,128,128,128,,silu,129280,163840,10000,MLA,RoPE (Rotary),RMSNorm,,,512,1536,,3,,"August 21, 2025",head_dim from v_head_dim=128; Has 1 shared expert; Uses KV LoRA rank 512; Uses Q LoRA rank 1536; Quantized fp8; RoPE scaling yarn; qk_nope_head_dim=128 qk_rope_head_dim=64,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
moonshotai/kimi-k2-thinking,Mixture-of-experts,0,0,0,0,0,"1,000,000,000,000","32,000,000,000","47,972,352","139,508,928.571429",0.036,0.125,0.438,61,,7168,18432,18432,2048,384,8,1,64,64,112,,silu,163840,262144,50000.0,MLA,RoPE (Rotary),RMSNorm,FALSE,,512,1536,,1,,"November 6, 2025",head_dim from v_head_dim=128; num_local_experts from n_routed_experts; Has 1 shared expert(s); Uses KV LoRA with rank 512; Uses Q LoRA with rank 1536; Quantized with compressed-tensors; RoPE scaling: yarn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,